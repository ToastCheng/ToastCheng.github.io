<!doctype html><html lang=en-us><head><title>Torchscript: Train in Python, Run in C++ |
ShihCheng Tu</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,viewport-fit=cover"><meta name=description content="This article is mainly about serialize a pytorch model, and load it into C++."><meta property="og:title" content="Torchscript: Train in Python, run in C++"><meta property="og:description" content="This article is mainly about serialize a pytorch model, and load it into C++."><meta property="og:type" content="article"><meta property="og:url" content="https://toastcheng.github.io/posts/2020-01-09-torchscript/"><meta property="article:published_time" content="2020-01-09T13:32:56+08:00"><meta property="article:modified_time" content="2020-01-09T13:32:56+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Torchscript: Train in Python, run in C++"><meta name=twitter:description content="This article is mainly about serialize a pytorch model, and load it into C++."><meta itemprop=name content="Torchscript: Train in Python, run in C++"><meta itemprop=description content="This article is mainly about serialize a pytorch model, and load it into C++."><meta itemprop=datePublished content="2020-01-09T13:32:56+08:00"><meta itemprop=dateModified content="2020-01-09T13:32:56+08:00"><meta itemprop=wordCount content="991"><meta itemprop=keywords content="torchscript,pytorch,c++,machine learning,"><link rel=canonical href=https://toastcheng.github.io/posts/2020-01-09-torchscript/><link rel=icon type=image/png href=https://toastcheng.github.ioimage/favicon.ico><link rel=stylesheet href=/css/font-awesome.min.css><link rel=stylesheet href=/css/bulma.min.css><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-168042428-1','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script src=/js/ramium.js></script><link rel=stylesheet href=/css/ramium.css></head><body><nav class="navbar is-dark" role=navigation aria-label="main navigation"><div class=navbar-brand><a class=navbar-item href=/><strong>ShihCheng Tu</strong></a>
<a role=button class="navbar-burger burger" aria-label=menu aria-expanded=false data-target=navbarBasicExample><span aria-hidden=true></span><span aria-hidden=true></span><span aria-hidden=true></span></a></div><div id=navbarBasicExample class=navbar-menu><div class=navbar-start><a class=navbar-item href=/>Home</a><div class="navbar-item has-dropdown is-hoverable"><a class=navbar-link>This Blog</a><div class=navbar-dropdown><a class=navbar-item href=/tags/>All Tags</a>
<a class=navbar-item href=/sections/>All Sections</a>
<a class=navbar-item href=/posts/>All Posts</a></div></div><a class=navbar-item href=/author/>Author</a></div><div class=navbar-end><a class="navbar-item navgithub" href=https://github.com/toastcheng/ target=_blank><i class="fa fa-github fa-2x"></i></a></div></div></nav><div class="columns is-centered"><div id=page-body class="column is-7"><div class="content blog"><h1>Torchscript: Train in Python, Run in C++</h1><div id=infobar class="level is-mobile"><div class=level-left><div class=level-item><p class="subtitle info date">Jan 9, 2020</p></div><div class=level-item><p class="subtitle info">6 mins read</p></div></div><div class="level-right is-hidden-touch"><div class=tags><a class="tag is-dark is-rounded" href=/tags/torchscript>Torchscript</a>
<a class="tag is-dark is-rounded" href=/tags/pytorch>Pytorch</a>
<a class="tag is-dark is-rounded" href=/tags/c++>C++</a>
<a class="tag is-dark is-rounded" href=/tags/machine-learning>Machine Learning</a></div></div></div><div class="tags is-hidden-desktop"><a class="tag is-dark is-rounded" href=/tags/torchscript>Torchscript</a>
<a class="tag is-dark is-rounded" href=/tags/pytorch>Pytorch</a>
<a class="tag is-dark is-rounded" href=/tags/c++>C++</a>
<a class="tag is-dark is-rounded" href=/tags/machine-learning>Machine learning</a></div><div class=blog-text><p>This article is mainly about serialize a pytorch model, and load it into C++.</p><h3 id=first-of-all-traced-your-model><strong>First of all, traced your model</strong></h3><p>&ldquo;Trace&rdquo; is a mechanism which pytorch used to record the behavior of neural nets.
To perform tracing, first you have to initialize your model, say that you have trained your model until it reach production level.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>import</span> torch
<span style=color:#f92672>from</span> your.model.path <span style=color:#f92672>import</span> Model

<span style=color:#75715e># assume that Model() is simply a network class which inherit nn.Module.</span>
model <span style=color:#f92672>=</span> Model()

<span style=color:#75715e># of course you have to load the weight of your model.</span>
weight <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>load(<span style=color:#e6db74>&#34;{your-weight-path}&#34;</span>, map_location<span style=color:#f92672>=</span>torch<span style=color:#f92672>.</span>device(<span style=color:#e6db74>&#34;cpu&#34;</span>))
model<span style=color:#f92672>.</span>load_state_dict(weight)
</code></pre></div><p>Then you have to make an <code>example</code>, i.e., a tensor that match the size of <code>model</code>&rsquo;s input size. Note that you will have to set <code>model</code> to <code>eval()</code> for disabling training features since we are doing this for inference.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>example <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>rand(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>512</span>, <span style=color:#ae81ff>512</span>)
model<span style=color:#f92672>.</span>eval()
</code></pre></div><p>After configuration, just simply called <code>torch.jit.trace</code> and pass the model and example to it. Once the tracing is done successfully, you can saved it into file.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>traced <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>jit<span style=color:#f92672>.</span>trace(model, example, check_trace<span style=color:#f92672>=</span>True)
traced<span style=color:#f92672>.</span>save(<span style=color:#e6db74>&#34;traced_model.pt&#34;</span>)
</code></pre></div><p>You can make a simple check to see if the output is the same as the original model.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>traced_model <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>jit<span style=color:#f92672>.</span>load(<span style=color:#e6db74>&#34;traced_model.pt&#34;</span>)

model(example)
<span style=color:#75715e># output: </span>
<span style=color:#75715e># tensor([[-10.5246, -15.4433, -15.8299, -14.9295, -13.4168, -10.2231]],</span>
<span style=color:#75715e>#        grad_fn=&lt;AddmmBackward&gt;)</span>
traced_model(example)
<span style=color:#75715e># output: </span>
<span style=color:#75715e># tensor([[-10.5246, -15.4433, -15.8299, -14.9295, -13.4168, -10.2231]],</span>
<span style=color:#75715e>#        grad_fn=&lt;AddmmBackward&gt;)</span>
</code></pre></div><p><em>note 1</em>
The traced torchscript file does not seems to be platform-independent, if you trace it on Mac, and load it in Windows, unpredictable errors happens when loading model.</p><p><em>note 2</em>
As offical document stated, if your model contains control flows(<code>if</code>, <code>else</code>), or any operations that depends on the value of <code>example</code>, i.e., <code>example</code> cannot reach some parts in your model, the traced model will not perform the same as the original model. To tackle this, you will need to add <code>@torch.jit.script</code> in every function which contains control flow.</p><p>e.g.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Model</span>(nn<span style=color:#f92672>.</span>Module):
    <span style=color:#f92672>...</span>
    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
        <span style=color:#f92672>...</span>
        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>_some_operation(x)
        <span style=color:#66d9ef>return</span> x
    <span style=color:#a6e22e>@torch.jit.script</span>
    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>_some_operation</span>(self, x):
        <span style=color:#66d9ef>if</span> x<span style=color:#f92672>.</span>sum() <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>:
            <span style=color:#f92672>...</span>
        <span style=color:#66d9ef>else</span>:
            <span style=color:#f92672>...</span>
</code></pre></div><p><em>note 3</em>
If your model contains custom <code>autograd.Function</code>, the operation cannot be traced (yet). One solution is to write a C++ operation and build pytorch by yourself like <a href=http://lernapparat.de/pytorch-traceable-differentiable/>this</a>, or you can simply try to use existed operations to build your custom operation rather than inherit <code>autograd.Function</code>.</p><h3 id=second-write-c-inference-code><strong>Second, write C++ inference code</strong></h3><p>Though the official document has made it pretty stright forward, I do have some hard time building it in Windows (no problem in unix-like). So I will focus on the steps that need to perform on Windows:</p><ol><li>Download source:<ul><li>cpu:</li></ul><pre><code>wget https://download.pytorch.org/libtorch/nightly/cpu/libtorch-shared-with-deps-latest.zip
</code></pre><ul><li>cuda 9.0:</li></ul><pre><code>wget https://download.pytorch.org/libtorch/nightly/cu90/libtorch-shared-with-deps-latest.zip
</code></pre></li><li>Preparing main.cpp, here I will extend the <a href=https://pytorch.org/tutorials/advanced/cpp_export.html>official example code</a>. The first part below is how c++ load a traced model, simply call <code>torch::jit::load()</code>.</li></ol><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;torch/script.h&gt;</span><span style=color:#75715e>
</span><span style=color:#75715e></span>
<span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;iostream&gt;</span><span style=color:#75715e>
</span><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;string.h&gt;</span><span style=color:#75715e>
</span><span style=color:#75715e></span>
<span style=color:#66d9ef>int</span> <span style=color:#a6e22e>main</span>() {
  
  std<span style=color:#f92672>::</span>string model_path <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;traced_model.pt&#34;</span>

  torch<span style=color:#f92672>::</span>jit<span style=color:#f92672>::</span>script<span style=color:#f92672>::</span>Module module;
  <span style=color:#66d9ef>try</span> {
    <span style=color:#75715e>// Deserialize the ScriptModule from a file using torch::jit::load().
</span><span style=color:#75715e></span>    module <span style=color:#f92672>=</span> torch<span style=color:#f92672>::</span>jit<span style=color:#f92672>::</span>load(model_path);
  }
  <span style=color:#66d9ef>catch</span> (<span style=color:#66d9ef>const</span> c10<span style=color:#f92672>::</span>Error<span style=color:#f92672>&amp;</span> e) {
    std<span style=color:#f92672>::</span>cerr <span style=color:#f92672>&lt;&lt;</span> <span style=color:#e6db74>&#34;error loading the model</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>;
    <span style=color:#66d9ef>return</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>;
  }

  ...
</code></pre></div><p>Say that you have an integer pointer (1-D array, flatten from a 2 or 3-D array) <code>int* data</code>, which is a way that C++ store image data. You can simply use <code>torch::from_blob()</code> to read the data into tensor, the sizes of it will be given in the second argument. The <code>options</code> is the way that <code>torch</code> used (as the third argument) to decide the type that will be read. You can find more in <a href=https://pytorch.org/cppdocs/notes/tensor_creation.html#configuring-properties-of-the-tensor>here</a>.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp>  ... (still in main())
  
  <span style=color:#66d9ef>int</span> height <span style=color:#f92672>=</span> <span style=color:#ae81ff>512</span>;
  <span style=color:#66d9ef>int</span> width <span style=color:#f92672>=</span> <span style=color:#ae81ff>512</span>;
  <span style=color:#66d9ef>int</span><span style=color:#f92672>*</span> data <span style=color:#f92672>=</span> get_image_data(); <span style=color:#75715e>// some api that you get your data.
</span><span style=color:#75715e></span>  
  at<span style=color:#f92672>::</span>Tensor t;
  <span style=color:#66d9ef>auto</span> options <span style=color:#f92672>=</span> c10<span style=color:#f92672>::</span>TensorOptions().dtype(torch<span style=color:#f92672>::</span>kShort);
  t <span style=color:#f92672>=</span> torch<span style=color:#f92672>::</span>from_blob(data, { 
    <span style=color:#75715e>/*batch=*/</span><span style=color:#ae81ff>1</span>, 
    <span style=color:#75715e>/*channel=*/</span><span style=color:#ae81ff>1</span>, 
    <span style=color:#75715e>/*width=*/</span>width, 
    <span style=color:#75715e>/*height=*/</span>height 
  }, options);
  
  ...
</code></pre></div><p>Lastly, you will have to initialize a vector of <code>torch::jit::IValue</code> as an input for a torchscript model. simply <code>push_back</code> a batch of tensor inside and you can run with <code>module.forward()</code>. You can get the result using <code>toTensor()</code>. There are some useful member functions of <code>Tensor</code>:</p><ul><li>observe the shape by <code>sizes()</code></li><li>cast the value into primitive type by <code>item&lt;type>()</code></li></ul><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp>  ... (still in main())
  
  std<span style=color:#f92672>::</span>vector<span style=color:#f92672>&lt;</span>torch<span style=color:#f92672>::</span>jit<span style=color:#f92672>::</span>IValue<span style=color:#f92672>&gt;</span> input;
  input.push_back(data)
  output <span style=color:#f92672>=</span> module.forward(input).toTensor();
  
  <span style=color:#75715e>// say that we want to know the output of the first input(0), second dimension(1)
</span><span style=color:#75715e></span>  std<span style=color:#f92672>::</span>cout <span style=color:#f92672>&lt;&lt;</span> <span style=color:#e6db74>&#34;result: &#34;</span> <span style=color:#f92672>&lt;&lt;</span> output[<span style=color:#ae81ff>0</span>][<span style=color:#ae81ff>1</span>].item<span style=color:#f92672>&lt;</span><span style=color:#66d9ef>float</span><span style=color:#f92672>&gt;</span>() <span style=color:#f92672>&lt;&lt;</span> std<span style=color:#f92672>::</span>endl;
  
  <span style=color:#75715e>// observe the shape
</span><span style=color:#75715e></span>  std<span style=color:#f92672>::</span>cout <span style=color:#f92672>&lt;&lt;</span> output.sizes() <span style=color:#f92672>&lt;&lt;</span> std<span style=color:#f92672>::</span>endl;
  
  <span style=color:#66d9ef>return</span> <span style=color:#ae81ff>0</span>;
}
</code></pre></div><h3 id=finally-build-the-app><strong>Finally, build the app</strong></h3><ol><li>Preparing cmakefile
We will have to write a CMakeLists to build the app. If you have not seen any <code>CMakeLists.txt</code> before it would be a little timidating, but it can be dissect:</li></ol><ul><li><code>project</code>
The name of the app, usually the name of root directory.</li><li><code>set</code>
Set any argument that is needed.</li><li><code>find_package</code>
Extremely handy if a third-party package has cmake support. cmake will find files like <code>{uppercase package name}Config.cmake</code> or <code>{lowercase package name}-config.cmake</code>. And You will not need to configure any lib path, include path by yourself. But you need to set <code>CMAKE_PREFIX_PATH</code> let cmake to know where to find those files. If you have multiple package you can set it like: <code>CMAKE_PREFIX_PATH={path 1}:{path 2}</code></li><li><code>add_executable</code>
Like <code>g++ -o test-torch main.cpp</code>.</li><li>The <code>MSVC</code> part
For Windows OS only, provided by document.</li></ul><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cmake data-lang=cmake>cmake_minimum_required(<span style=color:#e6db74>VERSION</span> <span style=color:#e6db74>3.0</span> <span style=color:#e6db74>FATAL_ERROR</span>)<span style=color:#960050;background-color:#1e0010>
</span><span style=color:#960050;background-color:#1e0010></span>project(<span style=color:#e6db74>test-torch</span>)<span style=color:#960050;background-color:#1e0010>
</span><span style=color:#960050;background-color:#1e0010>
</span><span style=color:#960050;background-color:#1e0010></span><span style=color:#75715e># if not set here, you should set it in argument with -D flag. e.g. -DCMAKE_PREFIX_PATH={libtorch path}
</span><span style=color:#75715e></span>set(<span style=color:#e6db74>CMAKE_PREFIX_PATH</span> <span style=color:#e6db74>&#34;{libtorch path}&#34;</span>)<span style=color:#960050;background-color:#1e0010>
</span><span style=color:#960050;background-color:#1e0010></span><span style=color:#75715e># if not set, annoying warnings will pop up
</span><span style=color:#75715e></span>cmake_policy(<span style=color:#e6db74>SET</span> <span style=color:#e6db74>CMP0054</span> <span style=color:#e6db74>NEW</span>)<span style=color:#960050;background-color:#1e0010>
</span><span style=color:#960050;background-color:#1e0010>
</span><span style=color:#960050;background-color:#1e0010></span>find_package(<span style=color:#e6db74>Torch</span> <span style=color:#e6db74>REQUIRED</span>)<span style=color:#960050;background-color:#1e0010>
</span><span style=color:#960050;background-color:#1e0010>
</span><span style=color:#960050;background-color:#1e0010></span>add_executable(<span style=color:#e6db74>test-torch</span> <span style=color:#e6db74>main.cpp</span>)<span style=color:#960050;background-color:#1e0010>
</span><span style=color:#960050;background-color:#1e0010></span>target_link_libraries(<span style=color:#e6db74>test-torch</span> <span style=color:#e6db74>&#34;${TORCH_LIBRARIES}&#34;</span>)<span style=color:#960050;background-color:#1e0010>
</span><span style=color:#960050;background-color:#1e0010></span>set_property(<span style=color:#e6db74>TARGET</span> <span style=color:#e6db74>test-torch</span> <span style=color:#e6db74>PROPERTY</span> <span style=color:#e6db74>CXX_STANDARD</span> <span style=color:#e6db74>14</span>)<span style=color:#960050;background-color:#1e0010>
</span><span style=color:#960050;background-color:#1e0010>
</span><span style=color:#960050;background-color:#1e0010></span>if (<span style=color:#e6db74>MSVC</span>)<span style=color:#960050;background-color:#1e0010>
</span><span style=color:#960050;background-color:#1e0010></span>  file(<span style=color:#e6db74>GLOB</span> <span style=color:#e6db74>TORCH_DLLS</span> <span style=color:#e6db74>&#34;${TORCH_INSTALL_PREFIX}/lib/*.dll&#34;</span>)<span style=color:#960050;background-color:#1e0010>
</span><span style=color:#960050;background-color:#1e0010></span>  add_custom_command(<span style=color:#e6db74>TARGET</span> <span style=color:#e6db74>test-torch</span>
                     <span style=color:#e6db74>POST_BUILD</span>
                     <span style=color:#e6db74>COMMAND</span> <span style=color:#f92672>${</span>CMAKE_COMMAND<span style=color:#f92672>}</span> <span style=color:#e6db74>-E</span> <span style=color:#e6db74>copy_if_different</span>
                     <span style=color:#f92672>${</span>TORCH_DLLS<span style=color:#f92672>}</span>
                     <span style=color:#f92672>$&lt;</span>TARGET_FILE_DIR:test-torch<span style=color:#f92672>&gt;</span>)<span style=color:#960050;background-color:#1e0010>
</span><span style=color:#960050;background-color:#1e0010></span>endif (<span style=color:#e6db74>MSVC</span>)<span style=color:#960050;background-color:#1e0010>
</span></code></pre></div><ol start=2><li>Build</li></ol><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>mkdir build
cd build
<span style=color:#75715e># set CMAKE_PREFIX_PATH to let cmake knows where to find the cmake file of libtorch.</span>
cmake -DCMAKE_PREFIX_PATH<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;{libtorch path}&#34;</span> ..
<span style=color:#75715e># OR if you set inside CMakeLists.txt you can alternatively run:</span>
cmake ..

<span style=color:#75715e># build, equivalent to &#34;make&#34;</span>
cmake --build . --config Release

<span style=color:#75715e># after building, dlls will be generated at the project root, </span>
<span style=color:#75715e># copy them to the path in where the executable is.</span>
cp c:<span style=color:#ae81ff>\.</span>..<span style=color:#ae81ff>\t</span>est-torch<span style=color:#ae81ff>\*</span>.dll C:<span style=color:#ae81ff>\.</span>..<span style=color:#ae81ff>\t</span>est-torch<span style=color:#ae81ff>\b</span>uild<span style=color:#ae81ff>\R</span>elease<span style=color:#ae81ff>\
</span><span style=color:#ae81ff></span>cd Release

<span style=color:#75715e># run!</span>
.<span style=color:#ae81ff>\t</span>est-torch.exe
</code></pre></div><p>reference</p><ul><li><a href=https://discuss.pytorch.org/t/error-running-libtorch-example-program/53980/6>build libtorch with cmake</a></li><li><a href=http://lernapparat.de/pytorch-traceable-differentiable/>custom op</a></li><li><a href=https://pytorch.org/tutorials/advanced/cpp_export.html>official c++ tutorial</a></li><li><a href=https://pytorch.org/cppdocs/notes/tensor_creation.html#configuring-properties-of-the-tensor>tensorOption</a></li></ul></div></div><div id=social-media-share class=has-text-centered><p><i>Sharing is caring!</i></p><br><div class=share-buttons><a href="https://www.facebook.com/sharer/sharer.php?u=https%3a%2f%2ftoastcheng.github.io%2fposts%2f2020-01-09-torchscript%2f" onclick="socialMediaPopUp(this.href,'',500,500);return false;" title="Share on Facebook. Opens in a new window."><img src=/img/icons/45px/facebook.png></a>
<a href="https://twitter.com/intent/tweet?text=Torchscript%3a%20Train%20in%20Python%2c%20run%20in%20C%2b%2b&url=https%3a%2f%2ftoastcheng.github.io%2fposts%2f2020-01-09-torchscript%2f" onclick="socialMediaPopUp(this.href,'',500,500);return false;" title="Share on Twitter. Opens in a new window."><img src=/img/icons/45px/twitter.png></a>
<a href="http://www.reddit.com/submit?url=https%3a%2f%2ftoastcheng.github.io%2fposts%2f2020-01-09-torchscript%2f" onclick="socialMediaPopUp(this.href,'',900,500);return false;" title="Share on Reddit. Opens in a new window."><img src=/img/icons/45px/reddit.png></a>
<a href="http://pinterest.com/pin/create/button/?url=https%3a%2f%2ftoastcheng.github.io%2fposts%2f2020-01-09-torchscript%2f" onclick="socialMediaPopUp(this.href,'',900,500);return false;" title="Share on Pinterest. Opens in a new window."><img src=/img/icons/45px/pinterest.png></a>
<a href="http://www.tumblr.com/share/link?url=https%3a%2f%2ftoastcheng.github.io%2fposts%2f2020-01-09-torchscript%2f" onclick="socialMediaPopUp(this.href,'',900,500);return false;" title="Share on Tumblr. Opens in a new window."><img src=/img/icons/45px/tumblr.png></a>
<a href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2ftoastcheng.github.io%2fposts%2f2020-01-09-torchscript%2f
			&title=Torchscript%3a%20Train%20in%20Python%2c%20run%20in%20C%2b%2b&summary=This%20article%20is%20mainly%20about%20serialize%20a%20pytorch%20model%2c%20and%20load%20it%20into%20C%2b%2b.&source=rafed123.github.io" onclick="socialMediaPopUp(this.href,'',900,500);return false;" title="Share on LinkedIn. Opens in a new window."><img src=/img/icons/45px/linkedin.png></a>
<a href="mailto:?subject=Torchscript%3a%20Train%20in%20Python%2c%20run%20in%20C%2b%2b&body=Check out this site https%3a%2f%2ftoastcheng.github.io%2fposts%2f2020-01-09-torchscript%2f" title="Share via Email. Opens in a new window."><img src=/img/icons/45px/mail.png></a></div></div><br><div id=disqus_thread></div><script type=text/javascript>(function(){if(window.location.hostname=="localhost")
return;var dsq=document.createElement('script');dsq.type='text/javascript';dsq.async=true;var disqus_shortname='toastcheng';dsq.src='//'+disqus_shortname+'.disqus.com/embed.js';(document.getElementsByTagName('head')[0]||document.getElementsByTagName('body')[0]).appendChild(dsq);})();</script><noscript>Please enable JavaScript to view the <a href=http://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript></div></div><footer class="footer has-background-dark"><div class="content has-text-centered has-text-white"><p>© 2020 Ramium. Powered by
<a class=has-text-light href=https://github.com/gohugoio/hugo target=_blank>Hugo</a>. Theme
<a class=has-text-light href=https://github.com/rafed123/ramium/ target=_blank>Ramium.</a></p></div></footer></body></html>